{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whale Sound Mining\n",
    "\n",
    "In this tutorial we will explore some data which contain right whale calls. The dataset was shared as part of a [2013 Kaggle competition](https://www.kaggle.com/c/whale-detection-challenge). Our goal is not to show the best winning algorithm to detect a call, but share a simple pipeline for processing oscillatory data, which possibly can be used on wide range of time series.\n",
    "\n",
    "Objectives:\n",
    "* read and extract features form audio data\n",
    "* apply dimensionality reduction techinques\n",
    "* unsupervised clustering \n",
    "* supervised classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing multiple visualization libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import mlab\n",
    "import pylab as pl\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing libraries to manipulate the data files\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import a library to read the .aiff format\n",
    "import aifc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The `train` folder contains many `.aiff` files (2 second snippets) and we have `.csv` document which contains the corresponding labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls whale_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = glob(os.path.join('whale_data','train','*.aiff'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are '+str(len(filenames))+' files.' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the labels\n",
    "import pandas as pd\n",
    "labels = pd.read_csv(os.path.join('whale_data','train.csv'), index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of the labels is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filenames which contain calls\n",
    "whale_labels = labels[labels['label'] == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whale_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are '+str(len(whale_labels))+' calls.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file info\n",
    "whale_sample_file = whale_labels[20] \n",
    "whale_aiff = aifc.open(os.path.join('whale_data','train',whale_sample_file),'r')\n",
    "print (\"Frames:\", whale_aiff.getnframes() )\n",
    "print (\"Frame rate (frames per second):\", whale_aiff.getframerate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "whale_strSig = whale_aiff.readframes(whale_aiff.getnframes())\n",
    "whale_array = np.fromstring(whale_strSig, np.short).byteswap()\n",
    "plt.plot(whale_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working directly with the signals is hard (there is important frequency information). Let's calculate the spectrogram for each of the signals and use them as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a function for plotting histograms\n",
    "def PlotSpecgram(P, freqs, bins):\n",
    "    \"\"\"Spectrogram\"\"\"\n",
    "    Z = np.flipud(P) # flip rows so that top goes to bottom, bottom to top, etc.\n",
    "    xextent = 0, np.amax(bins)\n",
    "    xmin, xmax = xextent\n",
    "    extent = xmin, xmax, freqs[0], freqs[-1]\n",
    "\n",
    "    im = pl.imshow(Z, extent=extent,cmap='viridis')\n",
    "    pl.axis('auto')\n",
    "    pl.xlim([0.0, bins[-1]])\n",
    "    pl.ylim([0, freqs[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'NFFT':256, 'Fs':2000, 'noverlap':192}\n",
    "P, freqs, bins = mlab.specgram(whale_array, **params)\n",
    "PlotSpecgram(P, freqs, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary which contains all the spectrograms\n",
    "spec_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cutting the higher frequencies ([:60,:])\n",
    "for filename in filenames[::1]:\n",
    "    aiff = aifc.open(filename,'r')\n",
    "    whale_strSig = aiff.readframes(aiff.getnframes())\n",
    "    whale_array = np.fromstring(whale_strSig, np.short).byteswap()\n",
    "    P, freqs, bins = mlab.specgram(whale_array, **params)\n",
    "    spec_dict[filename] = P[:60,:]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Most machine learning algorithms in Python expect the data to come in a format features x samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The key below depends on the directory structure\n",
    "feature_dict = {}\n",
    "for key in filenames[::1]:\n",
    "    feature_dict[key[17:]] = spec_dict[key].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del feature_dict\n",
    "del spec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now represent each signal with its spectrogram, which can be treated as a high dimensional vector. However, we can try to look for common patterns in those spectrograms and attempt to represent them with fewer dimensions. This can be accomplished by linear dimenionsality techniques such as Principal Component Analysis (PCA) and Independent Component Analysis (ICA). These techniques are easy to apply with the Python's `scikit-learn` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=30,copy=True)\n",
    "X_transformed = pca.fit_transform(X.iloc[:,:].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dimensions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did I pick 30 components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.singular_values_)\n",
    "plt.title(\"Singular values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first ten components\n",
    "plt.figure(figsize = (10,5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    # we are rescaling between 0 and 1 before plotting\n",
    "    u = pca.components_[i,:].reshape(-1,59)\n",
    "    #plt.imshow((u-u.min())/(u.max() - u.min()))\n",
    "    PlotSpecgram(u,freqs[:60],bins)\n",
    "    plt.title('Mode '+str(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ICA is expensive: we will look for 5 components\n",
    "# free some ram\n",
    "del X_transformed\n",
    "# run ica\n",
    "from sklearn.decomposition import FastICA\n",
    "ica = FastICA(n_components=5)\n",
    "X_transformed = ica.fit(X.T).transform(X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first 3 components\n",
    "plt.figure(figsize = (10,5))\n",
    "for i in range(5):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    # we are rescaling between 0 and 1 before plotting\n",
    "    u = ica.components_[i,:].reshape(-1,59)\n",
    "    plt.imshow((u-u.min())/(u.max() - u.min()))\n",
    "    PlotSpecgram(u,freqs[:60],bins)\n",
    "    plt.title('Mode '+str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Unsupervised Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to cluster the data and visualize in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of colors\n",
    "colors = []\n",
    "for c in labels['label'][X.columns]:\n",
    "    if c==1:\n",
    "        colors.append('red')\n",
    "    else:\n",
    "        colors.append('cyan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_transformed[:,1],X_transformed[:,0],[],color = colors)\n",
    "plt.title(\"Projection of the data on 2 components - ground truth labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of colors\n",
    "colors = []\n",
    "for c in kmeans.labels_:\n",
    "    if c==1:\n",
    "        colors.append('blue')\n",
    "    else:\n",
    "        colors.append('orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_transformed[:,1],X_transformed[:,0],[],color = colors)\n",
    "plt.title(\"Projection of the data on 2 components - kmeans labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few dimensions do not seem sufficient to separate the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Supervised Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this competition we have the privilige to have annotated data. So let's use those labels. We are in fact trying to solve a classification problem: we want to build a classifier which correctly identifies whale calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`scikit learn` has many built-in classification algorithms. Random Forests is a general purpose nonlinear classification method, which usually works pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install --yes scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the dataset in two parts: a training set and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X.T.iloc[:15000,:]\n",
    "Y_train = np.array(labels['label'][X.columns][:15000])\n",
    "X_test = X.T.iloc[15000:,:]\n",
    "Y_test = np.array(labels['label'][X.columns][15000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we fitted a classifier, but how should we evaluate performance? First let's look at the accuracy on the train dataset: it better be good!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction on the training dataset\n",
    "train_accuracy = 1 - np.sum(np.abs(clf.predict(X_train) - Y_train))/len(Y_train)\n",
    "print('Accuracy on the train dataset is '+ str(train_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what we want to know is how the method performs on the test dataset, whose labels we have not seen in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction on the testing dataset\n",
    "test_accuracy = 1 - np.sum(np.abs(clf.predict(X_test)-Y_test))/len(Y_test)\n",
    "print('Accuracy on the test dataset is '+ str(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, it is lower, as expected, but still decent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: what will be the accuracy if we always claim there is no whale call?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: what is the percentage of snippets with whale calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.sum()/len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Other measures: [Receiver Operating Characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) - Area Under the Curve (ROC AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The ROC curve allows explore how the algorithm performs for different decison thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_score = clf.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(Y_test, y_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are the low dimensional feautures better??? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the low dimensional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "clf.fit(X_transformed[:15000,:], np.array(labels['label'][X.columns][:15000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = 1 - np.sum(np.abs(clf.predict(X_transformed[:15000,:]) - labels['label'][X.columns][:15000]))/15000.\n",
    "print('Accuracy on the train dataset is '+ str(train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = 1 - np.sum(np.abs(clf.predict(X_transformed[15000:,:]) - labels['label'][X.columns][15000:]))/15000.\n",
    "print('Accuracy on the test dataset is '+ str(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How can we improve:\n",
    "\n",
    "* randomly split the train and test datasets\n",
    "\n",
    "* perform true cross validation\n",
    "\n",
    "* account for time shifting\n",
    "\n",
    "* equalize spectrograms\n",
    "\n",
    "* try other classifiers: gradient boosting, SVM\n",
    "\n",
    "* ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we have many many such files???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can speed up the computation of the spectrograms by parallelizing this step by file.\n",
    "* We will end up with a humongous features x samples array on which to run PCA.\n",
    "* If we can load this array in RAM, we can apply randomized PCA on it.\n",
    "* If not we can use dask to perform the PCA (SVD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put everything in one function\n",
    "def process_file(filename):\n",
    "    aiff = aifc.open(filename,'r')\n",
    "    whale_strSig = aiff.readframes(aiff.getnframes())\n",
    "    whale_array = np.fromstring(whale_strSig, np.short).byteswap()\n",
    "    P, freqs, bins = mlab.specgram(whale_array, **params)\n",
    "    return(P.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_file_dask = delayed(process_file)\n",
    "# create delayed evaluations\n",
    "lazy_values = [process_file_dask(filename) for filename in filenames]\n",
    "# create a list of dask arrays\n",
    "arrays = [da.from_delayed(lazy_value, shape=(7621,), dtype='float64') for lazy_value in lazy_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stack them all into one array\n",
    "stack = da.stack(arrays, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Do not execute those on jupyter hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# execute the computation\n",
    "# X = stack.compute()\n",
    "# compute in ram\n",
    "# u, s, vt = linalg.svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute out of core\n",
    "# u, s, vt = da.linalg.svd(stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### References:\n",
    "* https://github.com/jaimeps/whale-sound-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
